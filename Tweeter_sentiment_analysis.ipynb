{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e0cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5f7f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d796d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8a2457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be aware  dirty step to get money  #staylight #staywhite  #sarcastic #moralneeded @… https://t.co/Oj6BdyX3WG</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#sarcasm for #people who don't understand #diy #artattack http://t.co/rtyYmuDVUS</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IminworkJeremy @medsingle #DailyMail readers being sensible as always #shocker #sarcastic #dailyfail #inHuntspocket #theyhatethenhs</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@wilw Why do I get the feeling you like games? #sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-@TeacherArthurG @rweingarten You probably just missed the text. #sarcastic</td>\n",
       "      <td>figurative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 tweets  \\\n",
       "0                          Be aware  dirty step to get money  #staylight #staywhite  #sarcastic #moralneeded @… https://t.co/Oj6BdyX3WG   \n",
       "1                                                      #sarcasm for #people who don't understand #diy #artattack http://t.co/rtyYmuDVUS   \n",
       "2  @IminworkJeremy @medsingle #DailyMail readers being sensible as always #shocker #sarcastic #dailyfail #inHuntspocket #theyhatethenhs   \n",
       "3                                                                               @wilw Why do I get the feeling you like games? #sarcasm   \n",
       "4                                                           -@TeacherArthurG @rweingarten You probably just missed the text. #sarcastic   \n",
       "\n",
       "        class  \n",
       "0  figurative  \n",
       "1  figurative  \n",
       "2  figurative  \n",
       "3  figurative  \n",
       "4  figurative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49200b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1aa5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convrt into lower case\n",
    "df[\"tweets\"]=df[\"tweets\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a311328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tags\n",
    "def remove_html(text):\n",
    "    pattern=re.compile(\"<.*?>\")\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34118b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]=df[\"tweets\"].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b55312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls\n",
    "def remove_urls(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a75479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]=df[\"tweets\"].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa0579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8936\\3914944895.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"tweets\"]=df[\"tweets\"].str.replace(\"[^\\w\\s]\",'')\n"
     ]
    }
   ],
   "source": [
    "#remove puncutations\n",
    "df[\"tweets\"]=df[\"tweets\"].str.replace(\"[^\\w\\s]\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62fc93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words(\"english\")\n",
    "df[\"tweets\"]=df[\"tweets\"].apply(lambda x:\" \".join(x for x in x.split() if x not in stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a136ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emojis\n",
    "import emoji\n",
    "\n",
    "def deEmojis(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc5474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]=df[\"tweets\"].apply(deEmojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "593df3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8936\\1695435949.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"tweets\"]=df[\"tweets\"].str.replace(\"\\d+\",'')\n"
     ]
    }
   ],
   "source": [
    "#remove numerical values\n",
    "df[\"tweets\"]=df[\"tweets\"].str.replace(\"\\d+\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da342f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_tweets\"]=df[\"tweets\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fa47ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>token_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aware dirty step get money staylight staywhite sarcastic moralneeded</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sarcasm people dont understand diy artattack</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sarcasm, people, dont, understand, diy, artattack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iminworkjeremy medsingle dailymail readers sensible always shocker sarcastic dailyfail inhuntspocket theyhatethenhs</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wilw get feeling like games sarcasm</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[wilw, get, feeling, like, games, sarcasm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teacherarthurg rweingarten probably missed text sarcastic</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed, text, sarcastic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                tweets  \\\n",
       "0                                                 aware dirty step get money staylight staywhite sarcastic moralneeded   \n",
       "1                                                                         sarcasm people dont understand diy artattack   \n",
       "2  iminworkjeremy medsingle dailymail readers sensible always shocker sarcastic dailyfail inhuntspocket theyhatethenhs   \n",
       "3                                                                                  wilw get feeling like games sarcasm   \n",
       "4                                                            teacherarthurg rweingarten probably missed text sarcastic   \n",
       "\n",
       "        class  \\\n",
       "0  figurative   \n",
       "1  figurative   \n",
       "2  figurative   \n",
       "3  figurative   \n",
       "4  figurative   \n",
       "\n",
       "                                                                                                                      token_tweets  \n",
       "0                                                   [aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]  \n",
       "1                                                                              [sarcasm, people, dont, understand, diy, artattack]  \n",
       "2  [iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]  \n",
       "3                                                                                       [wilw, get, feeling, like, games, sarcasm]  \n",
       "4                                                                 [teacherarthurg, rweingarten, probably, missed, text, sarcastic]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eaac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c516687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]=df[\"tweets\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b9c463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>token_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sarcasm, people, dont, understand, diy, artattack]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sarcasm, people, dont, understand, diy, artattack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, reader, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wilw, get, feeling, like, game, sarcasm]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[wilw, get, feeling, like, games, sarcasm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed, text, sarcastic]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed, text, sarcastic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweets  \\\n",
       "0                                                  [aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]   \n",
       "1                                                                             [sarcasm, people, dont, understand, diy, artattack]   \n",
       "2  [iminworkjeremy, medsingle, dailymail, reader, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]   \n",
       "3                                                                                       [wilw, get, feeling, like, game, sarcasm]   \n",
       "4                                                                [teacherarthurg, rweingarten, probably, missed, text, sarcastic]   \n",
       "\n",
       "        class  \\\n",
       "0  figurative   \n",
       "1  figurative   \n",
       "2  figurative   \n",
       "3  figurative   \n",
       "4  figurative   \n",
       "\n",
       "                                                                                                                      token_tweets  \n",
       "0                                                   [aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]  \n",
       "1                                                                              [sarcasm, people, dont, understand, diy, artattack]  \n",
       "2  [iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]  \n",
       "3                                                                                       [wilw, get, feeling, like, games, sarcasm]  \n",
       "4                                                                 [teacherarthurg, rweingarten, probably, missed, text, sarcastic]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d60362b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>class</th>\n",
       "      <th>token_tweets</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]</td>\n",
       "      <td>aware dirty step get money staylight staywhite sarcastic moralneeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sarcasm, people, dont, understand, diy, artattack]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[sarcasm, people, dont, understand, diy, artattack]</td>\n",
       "      <td>sarcasm people dont understand diy artattack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, reader, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]</td>\n",
       "      <td>iminworkjeremy medsingle dailymail readers sensible always shocker sarcastic dailyfail inhuntspocket theyhatethenhs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wilw, get, feeling, like, game, sarcasm]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[wilw, get, feeling, like, games, sarcasm]</td>\n",
       "      <td>wilw get feeling like games sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed, text, sarcastic]</td>\n",
       "      <td>figurative</td>\n",
       "      <td>[teacherarthurg, rweingarten, probably, missed, text, sarcastic]</td>\n",
       "      <td>teacherarthurg rweingarten probably missed text sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           tweets  \\\n",
       "0                                                  [aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]   \n",
       "1                                                                             [sarcasm, people, dont, understand, diy, artattack]   \n",
       "2  [iminworkjeremy, medsingle, dailymail, reader, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]   \n",
       "3                                                                                       [wilw, get, feeling, like, game, sarcasm]   \n",
       "4                                                                [teacherarthurg, rweingarten, probably, missed, text, sarcastic]   \n",
       "\n",
       "        class  \\\n",
       "0  figurative   \n",
       "1  figurative   \n",
       "2  figurative   \n",
       "3  figurative   \n",
       "4  figurative   \n",
       "\n",
       "                                                                                                                      token_tweets  \\\n",
       "0                                                   [aware, dirty, step, get, money, staylight, staywhite, sarcastic, moralneeded]   \n",
       "1                                                                              [sarcasm, people, dont, understand, diy, artattack]   \n",
       "2  [iminworkjeremy, medsingle, dailymail, readers, sensible, always, shocker, sarcastic, dailyfail, inhuntspocket, theyhatethenhs]   \n",
       "3                                                                                       [wilw, get, feeling, like, games, sarcasm]   \n",
       "4                                                                 [teacherarthurg, rweingarten, probably, missed, text, sarcastic]   \n",
       "\n",
       "                                                                                                          clean_tweets  \n",
       "0                                                 aware dirty step get money staylight staywhite sarcastic moralneeded  \n",
       "1                                                                         sarcasm people dont understand diy artattack  \n",
       "2  iminworkjeremy medsingle dailymail readers sensible always shocker sarcastic dailyfail inhuntspocket theyhatethenhs  \n",
       "3                                                                                  wilw get feeling like games sarcasm  \n",
       "4                                                            teacherarthurg rweingarten probably missed text sarcastic  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_sentence(mylist):\n",
    "    return ' '.join(mylist)\n",
    "df['clean_tweets']=df['token_tweets'].apply(form_sentence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08fcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['class'].replace({\"figurative\":0,\"irony\":1,\"regular\":2,\"sarcasm\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c16ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['clean_tweets']\n",
    "y=df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75396eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d063b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65126,) (16282,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b7081",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "516b0a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oh': 114, 'look': 87, 'fun': 40, 'ironic': 70, 'love': 89, 'irony': 71, 'like': 82, 'sarcasm': 132, 'guy': 57, 'bad': 8, 'im': 67, 'today': 166, 'little': 83, 'us': 174, 'sure': 148, 'dont': 27, 'need': 106, 'thank': 156, 'would': 191, 'talk': 150, 'back': 7, 'via': 176, 'nice': 111, 'right': 129, 'going': 50, 'education': 29, 'thing': 161, 'free': 38, 'social': 142, 'game': 42, 'keep': 75, 'two': 173, 'making': 93, 'drugs': 28, 'politics': 123, 'news': 109, 'next': 110, 'cant': 16, 'make': 91, 'get': 43, 'tweet': 171, 'tech': 153, 'new': 108, 'video': 177, 'think': 163, 'know': 77, 'mean': 97, 'say': 135, 'isnt': 72, 'real': 127, 'funny': 41, 'didnt': 24, 'makes': 92, 'lol': 85, 'getting': 45, 'yet': 198, 'still': 146, 'doesnt': 25, 'big': 12, 'called': 15, 'people': 118, 'hate': 61, 'everyone': 33, 'tell': 154, 'school': 137, 'take': 149, 'day': 21, 'got': 54, 'years': 196, 'yes': 197, 'black': 13, 'thanks': 157, 'really': 128, 'top': 168, 'job': 74, 'thats': 158, 'one': 116, 'time': 165, 'shit': 139, 'women': 187, 'always': 2, 'guys': 58, 'work': 188, 'go': 48, 'money': 99, 'na': 104, 'last': 78, 'gopdebate': 53, 'gop': 52, 'phone': 120, 'yeah': 194, 'twitter': 172, 'theyre': 160, 'days': 22, 'tonight': 167, 'done': 26, 'late': 79, 'thought': 164, 'much': 101, 'better': 11, 'white': 185, 'great': 55, 'night': 112, 'someone': 143, 'could': 20, 'call': 14, 'anyone': 5, 'made': 90, 'long': 86, 'every': 32, 'peace': 117, 'said': 131, 'week': 183, 'man': 94, 'humor': 66, 'many': 95, 'well': 184, 'first': 37, 'must': 103, 'best': 10, 'wait': 178, 'happy': 59, 'trump': 169, 'amp': 3, 'wow': 192, 'watching': 181, 'also': 1, 'guess': 56, 'youre': 199, 'kids': 76, 'working': 189, 'hes': 64, 'rt': 130, 'life': 81, 'sarcastic': 133, 'good': 51, 'friends': 39, 'ive': 73, 'home': 65, 'ever': 31, 'internet': 69, 'something': 144, 'give': 46, 'lets': 80, 'wrong': 193, 'see': 138, 'old': 115, 'saw': 134, 'gets': 44, 'help': 63, 'way': 182, 'th': 155, 'media': 98, 'put': 125, 'trying': 170, 'says': 136, 'watch': 180, 'looks': 88, 'health': 62, 'person': 119, 'feel': 35, 'use': 175, 'nothing': 113, 'everything': 34, 'coming': 19, 'find': 36, 'god': 49, 'design': 23, 'talking': 151, 'things': 162, 'believe': 9, 'awesome': 6, 'please': 122, 'never': 107, 'even': 30, 'year': 195, 'name': 105, 'stop': 147, 'read': 126, 'want': 179, 'post': 124, 'live': 84, 'world': 190, 'another': 4, 'team': 152, 'play': 121, 'show': 140, 'sleep': 141, 'inspirational': 68, 'hard': 60, 'morning': 100, 'theres': 159, 'glad': 47, 'change': 17, 'music': 102, 'may': 96, 'start': 145, 'come': 18, 'without': 186, 'actually': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "vectorizer1 = TfidfVectorizer(max_features=200, ngram_range=(1,1))  \n",
    "tfidf_train1=vectorizer1.fit_transform(X_train).toarray()\n",
    "print(vectorizer1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ba6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love irony': 116, 'today sarcasm': 179, 'dont need': 37, 'would like': 192, 'back sarcasm': 4, 'right sarcasm': 154, 'love sarcasm': 119, 'game sarcasm': 59, 'hillary clinton': 77, 'didnt know': 29, 'makes sense': 120, 'lol sarcasm': 112, 'say sarcasm': 161, 'can not': 11, 'dont know': 34, 'gon na': 62, 'tonight sarcasm': 180, 'much better': 125, 'late night': 102, 'funny sarcasm': 56, 'dont think': 38, 'one irony': 137, 'fun sarcasm': 52, 'great sarcasm': 71, 'news politics': 130, 'politics intelmm': 150, 'intelmm osint': 88, 'donald trump': 31, 'cant wait': 15, 'humor funny': 80, 'last night': 100, 'ever sarcasm': 45, 'us sarcasm': 183, 'dont love': 36, 'sounds like': 167, 'oh irony': 135, 'new york': 129, 'meme humor': 122, 'funny pictures': 55, 'pictures pict': 146, 'pict funnypictures': 145, 'funnypictures funnytweets': 57, 'funnytweets humor': 58, 'gop debate': 67, 'first time': 51, 'politics health': 149, 'health entertainment': 75, 'funny humor': 54, 'didnt see': 30, 'coming sarcasm': 19, 'anyone else': 2, 'android design': 0, 'design android': 26, 'android education': 1, 'education design': 42, 'design tech': 28, 'tech longreads': 172, 'wan na': 185, 'time sarcasm': 176, 'isnt ironic': 94, 'social media': 166, 'sarcasm sarcastic': 160, 'awesome sarcasm': 3, 'well sarcasm': 188, 'sarcasm gopdebate': 157, 'pain painkillers': 139, 'painkillers drugs': 140, 'nothing like': 133, 'cant believe': 12, 'sarcasm lol': 159, 'seems like': 163, 'looks like': 115, 'many people': 121, 'good sarcasm': 65, 'people dont': 142, 'dont want': 39, 'know sarcasm': 98, 'irony lol': 93, 'good job': 63, 'types men': 182, 'men would': 123, 'would never': 193, 'never date': 128, 'date humor': 21, 'humor offbeat': 81, 'offbeat funny': 134, 'funny fun': 53, 'year sarcasm': 198, 'day ironic': 22, 'fair play': 47, 'love people': 118, 'much sarcasm': 127, 'politics news': 151, 'wordpress education': 190, 'dont even': 32, 'cant even': 13, 'love peace': 117, 'internet browsers': 89, 'browsers list': 9, 'list internet': 107, 'internet education': 90, 'education inspirational': 43, 'inspirational lol': 87, 'web design': 186, 'design hoting': 27, 'hoting hosting': 79, 'hosting education': 78, 'tech longrea': 171, 'good morning': 64, 'sarcasm humor': 158, 'dont like': 35, 'im sure': 86, 'irony gopdebate': 91, 'day sarcasm': 24, 'shocked sarcasm': 165, 'lol ironic': 110, 'lol irony': 111, 'time irony': 175, 'news rt': 131, 'guitar pick': 73, 'pick earrings': 144, 'politics dem': 148, 'dem gop': 25, 'peace love': 141, 'day irony': 23, 'year old': 197, 'irony life': 92, 'good thing': 66, 'night sarcasm': 132, 'go back': 61, 'thank god': 174, 'surprise sarcasm': 168, 'humor sarcasm': 82, 'im going': 84, 'look like': 113, 'customer service': 20, 'late post': 103, 'dont get': 33, 'work sarcasm': 191, 'im glad': 83, 'life sarcasm': 105, 'sarcasm bb': 156, 'years ago': 199, 'best sarcasm': 5, 'find writing': 49, 'writing center': 194, 'center writing': 16, 'writing education': 195, 'job sarcasm': 95, 'great day': 69, 'yay sarcasm': 196, 'looking forward': 114, 'better sarcasm': 6, 'drugs reuters': 41, 'tech longr': 170, 'rx drugs': 155, 'great job': 70, 'cant get': 14, 'joe moore': 96, 'moore quotes': 124, 'quotes humor': 152, 'see irony': 162, 'top blogger': 181, 'blogger templates': 8, 'templates blogger': 173, 'blogger education': 7, 'lol fu': 108, 'climate change': 17, 'gt tinder': 72, 'tinder fails': 177, 'fails humor': 46, 'kim davis': 97, 'feel like': 48, 'much fun': 126, 'glad see': 60, 'life irony': 104, 'one day': 136, 'got ta': 68, 'via youtube': 184, 'today irony': 178, 'ta love': 169, 'im shocked': 85, 'right irony': 153, 'like sarcasm': 106, 'high school': 76, 'first day': 50, 'dont worry': 40, 'last year': 101, 'lol funny': 109, 'well done': 187, 'employer code': 44, 'code business': 18, 'business education': 10, 'sense sarcasm': 164, 'ha ha': 74, 'one sarcasm': 138, 'people sarcasm': 143, 'labor day': 99, 'white people': 189, 'planned parenthood': 147}\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(max_features=200, ngram_range=(2,2)) \n",
    "tfidf_train2=vectorizer2.fit_transform(X_train).toarray()\n",
    "print(vectorizer2.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a68fe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'huge network politics': 91, 'washingtonpost news hnn': 188, 'vascable politics magazine': 186, 'meeting man dreams': 120, 'man dreams meeting': 119, 'dreams meeting beautiful': 31, 'ironic dont think': 105, 'news politics intelmm': 133, 'politics intelmm osint': 153, 'drugs druggists sundries': 33, 'druggists sundries wholesale': 32, 'via rfunny funny': 187, 'rfunny funny lol': 158, 'funny lol haha': 57, 'lol haha humor': 114, 'haha humor lmao': 84, 'humor lmao lmfao': 94, 'diet food yummy': 29, 'food yummy drugs': 48, 'yummy drugs pain': 199, 'drugs pain natural': 34, 'gop ccot gopdebate': 72, 'lol meme humor': 115, 'funny pictures pict': 59, 'pictures pict funnypictures': 150, 'pict funnypictures funnytweets': 148, 'funnypictures funnytweets humor': 62, 'im pretty sure': 99, 'news politics health': 132, 'politics health entertainment': 152, 'didnt see coming': 28, 'see coming sarcasm': 163, 'android design android': 0, 'design android education': 23, 'android education design': 1, 'education design tech': 40, 'design tech longreads': 27, 'pain painkillers drugs': 138, 'go wrong sarcasm': 69, 'types men would': 184, 'men would never': 121, 'would never date': 194, 'never date humor': 126, 'date humor offbeat': 20, 'humor offbeat funny': 96, 'offbeat funny fun': 136, 'fair play league': 44, 'wordpress scripts wordpress': 192, 'scripts wordpress education': 161, 'wordpress education design': 190, 'internet browsers list': 103, 'browsers list internet': 9, 'list internet education': 112, 'internet education inspirational': 104, 'education inspirational lol': 41, 'web design hoting': 189, 'design hoting hosting': 24, 'hoting hosting education': 90, 'hosting education design': 89, 'design tech longrea': 26, 'im gon na': 98, 'funny gifs gifs': 54, 'gifs gifs funnygifs': 67, 'gifs funnygifs funny': 66, 'funnygifs funny gif': 61, 'funny gif humor': 53, 'gif humor lol': 65, 'hippie peace guitar': 87, 'peace guitar pick': 141, 'guitar pick earrings': 82, 'pick earrings handmade': 146, 'politics dem gop': 151, 'news phone apple': 131, 'phone apple mobile': 145, 'bright purple peace': 8, 'purple peace sign': 154, 'peace sign stone': 142, 'sign stone earrings': 166, 'stone earrings fashion': 171, 'earrings fashion accessory': 37, 'fashion accessory shopping': 45, 'saw coming sarcasm': 160, 'th independence day': 178, 'dont even know': 30, 'peace symbol guitar': 143, 'symbol guitar pick': 176, 'pick earrings original': 147, 'gop dem politics': 74, 'find writing center': 46, 'writing center writing': 196, 'center writing education': 15, 'writing education inspirational': 197, 'news design tech': 128, 'painkillers drugs reuters': 139, 'stumbleupon perfect time': 174, 'perfect time social': 144, 'time social education': 180, 'social education design': 167, 'design tech longr': 25, 'cant wait see': 12, 'straight outta compton': 172, 'joe moore quotes': 107, 'moore quotes humor': 123, 'quotes humor quote': 155, 'theabcparty oz ir': 179, 'news market business': 130, 'lowest form wit': 116, 'top blogger templates': 182, 'blogger templates blogger': 7, 'templates blogger education': 177, 'blogger education inspirational': 6, 'inspirational lol fu': 101, 'gt tinder fails': 81, 'tinder fails humor': 181, 'study shows sarcasm': 173, 'sarcasm actually good': 159, 'funny pictures day': 58, 'pictures day pict': 149, 'day pict funnypictures': 21, 'good job sarcasm': 71, 'new york times': 127, 'keeps getting better': 109, 'friday giggle office': 52, 'giggle office ultimate': 68, 'office ultimate irony': 137, 'ultimate irony clickhere': 185, 'irony clickhere irony': 106, 'wtf news tv': 198, 'news tv fun': 135, 'tv fun funny': 183, 'start day sarcasm': 170, 'makes perfect sense': 117, 'last night sarcasm': 110, 'great way start': 80, 'host wordpress wordpress': 88, 'wordpress wordpress education': 193, 'got ta love': 77, 'much fun sarcasm': 124, 'national dog day': 125, 'search engine seo': 162, 'engine seo education': 43, 'seo education inspirational': 164, 'inspirational lol funny': 102, 'employer code business': 42, 'code business education': 17, 'business education design': 10, 'sydney news aus': 175, 'heal depression without': 85, 'depression without drugs': 22, 'cant wait sarcasm': 11, 'ha ha ha': 83, 'fresh writer writing': 51, 'writer writing education': 195, 'live breakingnews news': 113, 'great day sarcasm': 78, 'black lives matter': 5, 'gop ccot teaparty': 73, 'ccot teaparty gopdebate': 14, 'humor free ebook': 92, 'free ebook freebook': 49, 'ebook freebook freeebook': 39, 'freebook freeebook humor': 50, 'makes sense sarcasm': 118, 'gop gopdebate ccot': 75, 'gopdebate ccot tcot': 76, 'ccot tcot teaparty': 13, 'spoons need knife': 169, 'drugs reuters health': 35, 'news live breakingnews': 129, 'funny quotes pictures': 60, 'quotes pictures pict': 157, 'black coated metal': 4, 'coated metal peace': 16, 'metal peace earrings': 122, 'peace earrings great': 140, 'earrings great shopping': 38, 'great shopping gift': 79, 'shopping gift idea': 165, 'comment funny jokes': 18, 'funny jokes humor': 56, 'jokes humor oneliner': 108, 'quotes pict funnypictures': 156, 'life funny way': 111, 'health entertainment fashion': 86, 'social media irony': 168, 'news tech football': 134, 'get dumped acting': 64, 'dumped acting like': 36, 'humor joke funny': 93, 'anyone else see': 2, 'wordpress education inspirational': 191, 'better better sarcasm': 3, 'gon na get': 70, 'im shocked sarcasm': 100, 'funny humor lol': 55, 'humor lol meme': 95, 'funnytweets humor visit': 63, 'humor visit website': 97, 'customer service sarcasm': 19, 'first day school': 47}\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(max_features=200, ngram_range=(3,3)) \n",
    "tfidf_train3=vectorizer3.fit_transform(X_train).toarray()\n",
    "print(vectorizer3.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b331c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "# Fit and transform the vectorizers on the training data\n",
    "tfidf_train1 = vectorizer1.fit_transform(X_train)\n",
    "tfidf_train2 = vectorizer2.fit_transform(X_train)\n",
    "tfidf_train3 = vectorizer3.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizers\n",
    "tfidf_test1 = vectorizer1.transform(X_test)\n",
    "tfidf_test2 = vectorizer2.transform(X_test)\n",
    "tfidf_test3 = vectorizer3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64e752de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7407566638005159\n"
     ]
    }
   ],
   "source": [
    "# Combine the TF-IDF vectors\n",
    "combined_train = hstack([tfidf_train1, tfidf_train2, tfidf_train3]).toarray()\n",
    "combined_test = hstack([tfidf_test1, tfidf_test2, tfidf_test3]).toarray()\n",
    "\n",
    "# Create a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='newton-cg',random_state=42,C=0.5,n_jobs=-1)\n",
    "\n",
    "# Fit the model on the combined TF-IDF vectors\n",
    "model.fit(combined_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(combined_test)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = model.score(combined_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c25575bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.00      0.01      4179\n",
      "           1       0.67      0.99      0.80      4276\n",
      "           2       0.99      1.00      1.00      3696\n",
      "           3       0.66      0.99      0.80      4131\n",
      "\n",
      "    accuracy                           0.74     16282\n",
      "   macro avg       0.65      0.75      0.65     16282\n",
      "weighted avg       0.64      0.74      0.64     16282\n",
      "\n",
      "Confusion Matrix: \n",
      " [[  15 2075    7 2082]\n",
      " [  19 4248    9    0]\n",
      " [   0    1 3695    0]\n",
      " [  21    0    7 4103]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "cf_matrix=classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aac4ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6855423166687139\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.15      0.21      4179\n",
      "           1       0.66      0.85      0.74      4276\n",
      "           2       0.98      0.94      0.96      3696\n",
      "           3       0.65      0.83      0.73      4131\n",
      "\n",
      "    accuracy                           0.69     16282\n",
      "   macro avg       0.65      0.69      0.66     16282\n",
      "weighted avg       0.64      0.69      0.65     16282\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 642 1793   18 1726]\n",
      " [ 620 3632   23    1]\n",
      " [  52   87 3479   78]\n",
      " [ 706    1   15 3409]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Fit the model on the combined TF-IDF vectors\n",
    "model.fit(combined_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred1 = model.predict(combined_test)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = model.score(combined_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "# Generate confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "957251fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6549563935634443\n",
      "Confusion Matrix: \n",
      " [[ 131 2030    3 2015]\n",
      " [ 853 3420    3    0]\n",
      " [   0    3 3693    0]\n",
      " [ 705    3    3 3420]]\n",
      "classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.03      0.04      4179\n",
      "           1       0.63      0.80      0.70      4276\n",
      "           2       1.00      1.00      1.00      3696\n",
      "           3       0.63      0.83      0.72      4131\n",
      "\n",
      "    accuracy                           0.65     16282\n",
      "   macro avg       0.58      0.66      0.62     16282\n",
      "weighted avg       0.57      0.65      0.60     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200,min_samples_split=10,\n",
    "                                  criterion='gini',\n",
    "                                  bootstrap=True, \n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1)\n",
    "rf_model.fit(combined_train, y_train)\n",
    "\n",
    "y_pred2 = rf_model.predict(combined_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, y_pred2)\n",
    "print(\"Confusion Matrix: \\n\", confusion_mat)\n",
    "\n",
    "classification_report = classification_report(y_test, y_pred2)\n",
    "print(\"classification_report: \\n\",classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f1836ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "tweets=df['clean_tweets']\n",
    "\n",
    "story=[]\n",
    "for tweet in tweets:\n",
    "    raw_sent=sent_tokenize(tweet)\n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a2f840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "model12=gensim.models.Word2Vec( window=11,\n",
    "                             min_count=2,\n",
    "                             vector_size=300,\n",
    "                             alpha=0.025,  \n",
    "                             negative=20,\n",
    "                             workers=cores-1,\n",
    "                             sg=0\n",
    "                             )\n",
    "#sg=0 for skipgram and 1for cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba810b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "373f5c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81404"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbf0c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9797502, 11414955)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(story,total_examples=model.corpus_count,epochs=15,compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79bab74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39378"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be99af78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('offbeat', 0.7222816348075867),\n",
       " ('types', 0.6945273876190186),\n",
       " ('bachelorau', 0.6611188650131226),\n",
       " ('sonofabitch', 0.6146183013916016),\n",
       " ('ladies', 0.5906879901885986),\n",
       " ('guessed', 0.5736902952194214),\n",
       " ('tgit', 0.5725183486938477),\n",
       " ('fucken', 0.5644412636756897),\n",
       " ('astound', 0.5620062351226807),\n",
       " ('fairs', 0.561787486076355)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19e3879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out=open(\"vectorizer_model.pkl\",\"wb\")\n",
    "pickle.dump(model12,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9590e4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50383615"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('good','bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b758f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 81408/81408 [05:49<00:00, 232.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def document_vector(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc.split(' ') if word in model.wv.index_to_key]\n",
    "    \n",
    "    if not doc:  # Check if the doc list is empty\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector or an appropriate default value\n",
    "    \n",
    "    return np.mean(model.wv[doc], axis=0)\n",
    "\n",
    "X = []\n",
    "\n",
    "for doc in tqdm(tweets):\n",
    "    if isinstance(doc, float):\n",
    "        doc = str(doc)\n",
    "    \n",
    "    X.append(document_vector(doc))\n",
    "\n",
    "X = np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9fae7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is:  0.5944601400319371\n",
      "Classifiaction report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.02      0.02      4179\n",
      "           1       0.58      0.66      0.62      4276\n",
      "           2       0.97      0.99      0.98      3696\n",
      "           3       0.60      0.75      0.67      4131\n",
      "\n",
      "    accuracy                           0.59     16282\n",
      "   macro avg       0.54      0.61      0.57     16282\n",
      "weighted avg       0.53      0.59      0.56     16282\n",
      "\n",
      "Confusion matrix: \n",
      " [[  66 2049   35 2029]\n",
      " [1377 2836   50   13]\n",
      " [   4    3 3664   25]\n",
      " [ 965    7   46 3113]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# class_weights = {0: 5, 1: 3, 2: 3, 3: 3}\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=300,random_state=42,n_jobs=-1)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred=rf.predict(X_test)\n",
    "\n",
    "score=accuracy_score(y_test,y_pred)\n",
    "print('Accuracy Score is: ', score)\n",
    "\n",
    "print('Classifiaction report: ',classification_report(y_test,y_pred))\n",
    "print('Confusion matrix: \\n',confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f8f78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354747574008107\n",
      "Classification Report: \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.36      0.06       413\n",
      "           1       0.97      0.67      0.79      6180\n",
      "           2       1.00      1.00      1.00      3703\n",
      "           3       0.96      0.66      0.79      5986\n",
      "\n",
      "    accuracy                           0.74     16282\n",
      "   macro avg       0.74      0.67      0.66     16282\n",
      "weighted avg       0.95      0.74      0.82     16282\n",
      "\n",
      "confusion matrix: \n",
      "  [[ 147  116    0  150]\n",
      " [2024 4154    1    1]\n",
      " [   1    6 3695    1]\n",
      " [2007    0    0 3979]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3.1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model\n",
    "model_ = LogisticRegression()\n",
    "\n",
    "# Fit the model on the combined TF-IDF vectors\n",
    "model_.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model_.predict(X_test)\n",
    "\n",
    "# Convert Series objects to NumPy arrays and reshape\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = model_.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print('Classification Report: \\n ',classification_report(y_pred,y_test))\n",
    "print('confusion matrix: \\n ',confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dc81f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354133398845351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a BaLoggging Classifier\n",
    "base_model =LogisticRegression()\n",
    "bag_model = BaggingClassifier(base_estimator=base_model,n_jobs=-1,n_estimators=20,random_state=42,verbose=2)\n",
    "\n",
    "# Fit the model on the training data\n",
    "bag_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bag_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out=open(\"model_.pkl\",\"wb\")\n",
    "pickle.dump(bag_model,pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
